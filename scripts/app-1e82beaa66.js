!function(){"use strict";angular.module("nii-website",["ngAnimate","ngCookies","ngTouch","ngSanitize","ngMessages","ngAria","ngResource","ui.router","toastr"])}(),function(){"use strict";function e(){function e(){return t}var t=[{title:"AngularJS",url:"https://angularjs.org/",description:"HTML enhanced for web apps!",logo:"angular.png"},{title:"BrowserSync",url:"http://browsersync.io/",description:"Time-saving synchronised browser testing.",logo:"browsersync.png"},{title:"GulpJS",url:"http://gulpjs.com/",description:"The streaming build system.",logo:"gulp.png"},{title:"Jasmine",url:"http://jasmine.github.io/",description:"Behavior-Driven JavaScript.",logo:"jasmine.png"},{title:"Karma",url:"http://karma-runner.github.io/",description:"Spectacular Test Runner for JavaScript.",logo:"karma.png"},{title:"Protractor",url:"https://github.com/angular/protractor",description:"End to end test framework for AngularJS applications built on top of WebDriverJS.",logo:"protractor.png"},{title:"Bootstrap",url:"http://getbootstrap.com/",description:"Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.",logo:"bootstrap.png"}];this.getTec=e}angular.module("nii-website").service("webDevTec",e)}(),function(){"use strict";function e(e){function t(){return i}var o=e.getImages(),i=[{title:"Wearable Visual Aid",tags:["iOS","C++","Computer Vision","Algorithms","PhD"],date:"Fall 2015 - Spring 2016",description:"An AI based grocery shopping assistant",url:"",id:"wva",logo:o.wva.logo.square,figures:o.wva.figures,template:"app/model/projects/wva.html"},{title:"Proxee",tags:["iOS","mobile"],date:"June 2015",description:"An iOS app for finding apartments and roommates",url:"",id:"proxee",logo:o.proxee.logo.square,figures:o.proxee.figures,template:"app/model/projects/proxee.html"},{title:"Dedup",date:"Fall 2015",tags:["Python","Algorithms"],description:"Multi process image de-duplication in python",url:"https://github.com/nmante/image_deduplication",id:"dedup",logo:o.dedup.logo.square,figures:o.dedup.figures,template:"app/model/projects/dedup.html"},{title:"MEGA",date:"November 2013",tags:["HTML","CSS","JavaScript","Web"],description:"A mockup website for MEGA",url:"https://github.com/nmante/MEGA_Website",id:"mega",logo:o.mega.logo.square,figures:o.mega.figures,template:"app/model/projects/mega.html"},{title:"Lil Tron",date:"April 2015",tags:["AngularJS","NodeJS","Python","AWS"],description:"An artificially intelligent rap lyric bot",url:"",id:"liltron",logo:o.liltron.logo.square,figures:o.liltron.figures,template:"app/model/projects/liltron.html"},{title:"OLTS",date:"2012 - 2015",tags:["C++","Computer Vision","Algorithms","PhD"],description:"Real time object tracking program for the blind",url:"https://github.com/nmante/olts",id:"olts",logo:o.olts.logo.square,figures:o.olts.figures,template:"app/model/projects/olts.html"},{title:"visor.AI",date:"Spring 2016",tags:["Deep Learning","Algorithms","Computer Vision","Torch","PhD"],description:"Deep learning for grocery item recognition",url:"",id:"visor",logo:o.visor.logo.square,figures:o.visor.figures,template:"app/model/projects/visor.html"}];this.getProjects=t}e.$inject=["imageService"],angular.module("nii-website").service("projectService",e)}(),function(){"use strict";angular.module("nii-website").directive("mainNavbar",function(){function e(e,t){var o=this;o.url=t.absUrl(),o.relativeDate=e(o.creationDate).fromNow(),angular.element("#navbar a, body").click(function(){angular.element("#navbar").collapse("hide")})}e.$inject=["moment","$location"];var t={restrict:"E",templateUrl:"app/components/navbar/navbar.html",scope:{creationDate:"="},controller:e,controllerAs:"vm",bindToController:!0};return t})}(),function(){"use strict";function e(e){function t(t,o,i,a){var s,r=e(o[0],{typeSpeed:40,deleteSpeed:40,pauseDelay:800,loop:!0,postfix:" "});o.addClass("acme-malarkey"),angular.forEach(t.extraValues,function(e){r.type(e).pause()["delete"]()}),s=t.$watch("vm.contributors",function(){angular.forEach(a.contributors,function(e){r.type(e.login).pause()["delete"]()})}),t.$on("$destroy",function(){s()})}function o(e,t){function o(){return i().then(function(){e.info("Activated Contributors View")})}function i(){return t.getContributors(10).then(function(e){return a.contributors=e,a.contributors})}var a=this;a.contributors=[],o()}o.$inject=["$log","githubContributor"];var i={restrict:"E",scope:{extraValues:"="},template:"&nbsp;",link:t,controller:o,controllerAs:"vm"};return i}e.$inject=["malarkey"],angular.module("nii-website").directive("acmeMalarkey",e)}(),function(){"use strict";function e(){function e(){return o}var t=this,o={index:{niiProfile:"https://c5.staticflickr.com/6/5467/29492803884_dcfcf4ca8e_o.jpg",niiWebShare:"https://c3.staticflickr.com/6/5021/29492803754_2431345961_o.jpg"},main:{about:{airplane:"https://c1.staticflickr.com/9/8401/29492808024_0258485173_o.png",soccer:"https://c2.staticflickr.com/6/5280/29492807074_bae6042eb2_o.png",computer:"https://c2.staticflickr.com/6/5629/29492804144_101b4c3ee6_o.png",cloud:"https://c1.staticflickr.com/9/8540/29492807904_6953b5a150_o.png"}},wva:{logo:{thumbnail:"https://c1.staticflickr.com/8/7520/29492797424_ff7e3d8329_t.jpg",small:"https://c1.staticflickr.com/8/7520/29492797424_ff7e3d8329_n.jpg",medium:"https://c1.staticflickr.com/8/7520/29492797424_ff7e3d8329_c.jpg",large:"https://c1.staticflickr.com/8/7520/29492797424_ff7e3d8329_b.jpg",square:"https://c1.staticflickr.com/8/7520/29492797424_ff7e3d8329_q.jpg"},masthead:"",figures:{demo:{small:"",original:"https://c2.staticflickr.com/6/5515/29492798394_87bddb1c4c_o.png"},flowChart:{medium:"https://c1.staticflickr.com/8/7520/29492797424_ff7e3d8329_z.jpg",original:"https://c2.staticflickr.com/8/7520/29492797424_a4e3cd114c_o.png"},iOS:{medium:"https://c7.staticflickr.com/9/8407/29492797574_b944303430_c.jpg",original:"https://c1.staticflickr.com/9/8407/29492797574_72ee5353d4_o.png"},sensory:{small:"",original:"https://c3.staticflickr.com/6/5233/29492799234_d11c7b2330_o.jpg"}}},proxee:{logo:{square:"https://c6.staticflickr.com/9/8396/29492284413_0042f98cf6_q.jpg",thumbnail:"https://c6.staticflickr.com/9/8396/29492284413_0042f98cf6_t.jpg",small:"https://c6.staticflickr.com/9/8396/29492284413_0042f98cf6_n.jpg",medium:"https://c6.staticflickr.com/9/8396/29492284413_0042f98cf6_c.jpg",large:"https://c6.staticflickr.com/9/8396/29492284413_6ee096569e_k.jpg",original:""},masthead:"https://c5.staticflickr.com/6/5443/29492808084_0a67112a39_h.jpg",figures:{listing:{large:"https://c6.staticflickr.com/9/8396/29492284413_bcb16029ce_h.jpg"},roommate:{large:"https://c5.staticflickr.com/6/5651/29492799644_c78c674078_h.jpg"}}},dedup:{logo:{square:"https://c5.staticflickr.com/9/8617/29492807804_d30beb4787_q.jpg",thumbnail:"https://c5.staticflickr.com/9/8617/29492807804_d30beb4787_t.jpg",small:"https://c5.staticflickr.com/9/8617/29492807804_d30beb4787_n.jpg",medium:"https://c5.staticflickr.com/9/8617/29492807804_d30beb4787.jpg",large:"https://c5.staticflickr.com/9/8617/29492807804_d30beb4787.jpg",original:"https://c5.staticflickr.com/9/8617/29492807804_d30beb4787.jpg"},figures:{flowDiagram:{medium:"https://c3.staticflickr.com/6/5207/29492807634_754a79600e_z.jpg",large:"https://c3.staticflickr.com/6/5207/29492807634_754a79600e_b.jpg",original:"https://c3.staticflickr.com/6/5207/29492807634_e83294f3d5_o.jpg"}}},mega:{logo:{square:"https://c7.staticflickr.com/9/8723/29492805934_0defdda8ed_q.jpg",thumbnail:"https://c7.staticflickr.com/9/8723/29492805934_0defdda8ed_t.jpg",small:"https://c7.staticflickr.com/9/8723/29492805934_0defdda8ed_n.jpg",medium:"https://c7.staticflickr.com/9/8723/29492805934_0defdda8ed_z.jpg",original:"https://c7.staticflickr.com/9/8723/29492805934_e041ab5d7f_o.jpg"},masthead:"https://c5.staticflickr.com/6/5308/29492805564_8c1f2c3317_c.jpg",figures:{fullWeb:{medium:"https://c5.staticflickr.com/6/5308/29492805564_8c1f2c3317_c.jpg",original:"https://c5.staticflickr.com/6/5308/29492805564_3dd2ba882f_o.jpg"}}},liltron:{logo:{square:"https://c1.staticflickr.com/6/5481/29492806864_bff71088c0_q.jpg",thumbnail:"https://c1.staticflickr.com/6/5481/29492806864_bff71088c0_t.jpg",small:"https://c1.staticflickr.com/6/5481/29492806864_bff71088c0_n.jpg",medium:"https://c1.staticflickr.com/6/5481/29492806864_bff71088c0_z.jpg",large:"https://c1.staticflickr.com/6/5481/29492806864_bff71088c0_b.jpg",original:"https://c1.staticflickr.com/6/5481/29492806864_5a6269e356_o.jpg"},masthead:"https://c5.staticflickr.com/9/8545/29492806604_b02c39356a_c.jpg",figures:{iphone:{medium:"https://c1.staticflickr.com/6/5481/29492806864_bff71088c0_z.jpg",large:"https://c1.staticflickr.com/6/5481/29492806864_bff71088c0_b.jpg",original:"https://c1.staticflickr.com/6/5481/29492806864_5a6269e356_o.jpg"},full:{medium:"https://c5.staticflickr.com/9/8545/29492806604_b02c39356a_c.jpg",large:"https://c5.staticflickr.com/9/8545/29492806604_b02c39356a_b.jpg",original:"https://c5.staticflickr.com/9/8545/29492806604_3ff3044011_o.jpg"}}},olts:{logo:{square:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1_q.jpg",thumbnail:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1_t.jpg",small:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1_n.jpg",medium:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1.jpg",original:"https://c3.staticflickr.com/6/5233/29492799234_d11c7b2330_o.jpg"},masthead:"https://c3.staticflickr.com/6/5503/29492797914_e134490667_o.jpg",figures:{flowchart:{square:"https://c3.staticflickr.com/6/5503/29492797914_430587f854_s.jpg",small:"https://c3.staticflickr.com/6/5503/29492797914_430587f854_n.jpg",original:"https://c3.staticflickr.com/6/5503/29492797914_e134490667_o.jpg"},sensory:{square:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1_q.jpg",thumbnail:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1_t.jpg",small:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1_n.jpg",medium:"https://c3.staticflickr.com/6/5233/29492799234_4c6d354af1.jpg",original:"https://c3.staticflickr.com/6/5233/29492799234_d11c7b2330_o.jpg"}}},visor:{logo:{square:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_q.jpg",thumbnail:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_t.jpg",small:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_n.jpg",medium:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_c.jpg",large:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_b.jpg",original:"https://c3.staticflickr.com/6/5453/29492798914_83319df127_o.jpg"},masthead:"https://c3.staticflickr.com/6/5453/29492798914_83319df127_o.jpg",figures:{network:{square:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_q.jpg",thumbnail:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_t.jpg",small:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_n.jpg",medium:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_c.jpg",large:"https://c3.staticflickr.com/6/5453/29492798914_88ff4e93ef_b.jpg",original:"https://c3.staticflickr.com/6/5453/29492798914_83319df127_o.jpg"}}}};t.getImages=e}angular.module("nii-website").service("imageService",e)}(),function(){"use strict";function e(e,t){function o(o){function a(e){return e.data}function s(t){e.error("XHR Failed for getContributors.\n"+angular.toJson(t.data,!0))}return o||(o=30),t.get(i+"/contributors?per_page="+o).then(a)["catch"](s)}var i="https://api.github.com/repos/Swiip/generator-gulp-angular",a={apiHost:i,getContributors:o};return a}e.$inject=["$log","$http"],angular.module("nii-website").factory("githubContributor",e)}(),function(){"use strict";function e(){}angular.module("nii-website").service("blogService",e)}(),function(){"use strict";function e(){}angular.module("nii-website").controller("ResumeController",e)}(),function(){"use strict";function e(e,t,o){for(var i=this,a=o.getProjects(),s=0;s<a.length;s++){var r=a[s];r.id.toLowerCase()==t.projectId.toLowerCase()&&(i.project=r)}}e.$inject=["$timeout","$stateParams","projectService"],angular.module("nii-website").controller("ProjectsController",e)}(),function(){"use strict";function e(e,t,o,i,a,s){function r(e){e=""===e?"masthead":e.length>3?e.indexOf("#/#")>-1?e.replace("#/#",""):""+e:"masthead",angular.element("html,body").animate({scrollTop:angular.element("#"+e).offset().top},"slow")}var l=this,c=i.getImages();e.$on("$stateChangeSuccess",function(){if(1==s.url().indexOf("home")){var e=s.url().split("/").pop();"home"===e&&(e="masthead"),o(function(){r(e)},100)}}),l.abouts=[{img:c.main.about.computer,alt:"mobile-web",title:"FRONT END",text:"I love spending my spare time building iOS apps and websites."},{img:c.main.about.cloud,alt:"algos",title:"BACK END",text:"I've written NodeJS, Python, or even deep learning and computer vision programs."},{img:c.main.about.airplane,alt:"travel",title:"TRAVELER",text:"Traveling is a passion of mine! I've visited Europe, Africa, South America and I hope to add more to the list soon."},{img:c.main.about.soccer,alt:"sports",title:"SPORTS",text:"I love playing soccer and basketball. I was even fortunate enough to go to the Brazil World Cup!"}],l.projects=a.getProjects();for(var n=0;n<l.projects.length;n++)for(var d="",h=l.projects[n].tags,p=0;p<h.length;p++)d+="<a ng-src=''>"+h[p]+"</a>",p!=h.length-1&&(d+=", "),l.projects[n].tagline=d;l.socials=[{name:"Github",id:"social-icon",link:"http://www.github.com/nmante",icon:"fa fa-github"},{name:"Twitter",id:"social-icon",link:"http://www.twitter.com/niimante",icon:"fa fa-twitter"},{name:"LinkedIn","class":"social-icon",link:"http://www.linkedin.com/in/nii-mante-phd-6a4a2745",icon:"fa fa-linkedin"}]}e.$inject=["$scope","$rootScope","$timeout","imageService","projectService","$location"],angular.module("nii-website").controller("MainController",e)}(),function(){"use strict";function e(){}angular.module("nii-website").controller("BlogController",e)}(),function(){"use strict";function e(){}angular.module("nii-website").run(e)}(),function(){"use strict";function e(e,t){e.state("home",{url:"/home",templateUrl:"app/main/main.html",controller:"MainController",controllerAs:"main"}).state("home.masthead",{url:"/masthead",templateUrl:"app/main/main.html",controller:"MainController",controllerAs:"main",params:{location:"masthead"}}).state("home.about",{url:"/about",templateUrl:"app/main/main.html",controller:"MainController",controllerAs:"main",params:{location:"about"}}).state("home.project",{url:"/project",templateUrl:"app/main/main.html",controller:"MainController",controllerAs:"main",params:{location:"project"}}).state("home.contact",{url:"/contact",templateUrl:"app/main/main.html",controller:"MainController",controllerAs:"main"}).state("projects",{url:"/projects/:projectId",templateUrl:"app/projects/projects.html",controller:"ProjectsController",controllerAs:"projectsController"}).state("blog",{url:"/blog",templateUrl:"app/blog/blog.html",controller:"BlogController",controllerAs:"blogsController"}).state("resume",{url:"/resume",templateUrl:"app/resume/resume.html",controller:"ResumeController",controllerAs:"resumeController"}),t.otherwise("/home")}e.$inject=["$stateProvider","$urlRouterProvider"],angular.module("nii-website").config(e)}(),function(){"use strict";angular.module("nii-website").constant("malarkey",malarkey).constant("moment",moment)}(),function(){"use strict";function e(e,t){e.debugEnabled(!0),t.allowHtml=!0,t.timeOut=3e3,t.positionClass="toast-top-right",t.preventDuplicates=!0,t.progressBar=!0}e.$inject=["$logProvider","toastrConfig"],angular.module("nii-website").config(e)}(),angular.module("nii-website").run(["$templateCache",function(e){e.put("app/blog/blog.html",""),e.put("app/main/main.html",'<div id=masthead class="jumbotron vertical-center text-center"><div id=banner class=container><!-- wow fadeInDown --><h2 class="wow fadeInDown">Nii Mante, PhD</h2><!-- wow fadeInDown --><h4 class="wow fadeInDown">Software Engineer @ <a target=_blank href=http://www.buzzfeed.com id=buzzfeed-brand>BuzzFeed</a></h4><!-- wow fadeInUp --><h4 class="socials wow fadeInUp"><span ng-repeat="social in main.socials"><a href={{social.link}} target=_blank><i id={{social.name}} class={{social.icon}}></i></a></span></h4></div></div><div class=container-fluid><!--  wow fadeIn --><div id=about class="row text-center wow fadeIn"><br><br><h3>ABOUT NII</h3><div class="col-sm-2 col-xs-1 col-md-3 col-lg-3"></div><div class="col-sm-8 col-xs-10 col-md-6 col-lg-6 text-justify"><br><p>I\'m Nii. I completed my MS & PhD in Biomedical Engineering at the University of Southern California (USC) in August 2016. I also completed my Masters in Computer Science at USC in December 2015. My research dealt with building computer vision systems to help the blind. I graduated from the University of Maryland in 2010 with a BS in Bioengineering.</p><br><br><br></div><div class="col-sm-2 col-xs-1 col-md-3 col-lg-3"></div><br><br><br></div><div class="row text-center"><div class="col-md-1 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-10 col-lg-8"><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6 text-center" ng-repeat="about in main.abouts"><img ng-src={{about.img}} alt="{{ about.alt }}"><br><br><h4>{{ about.title }}</h4><p class=about-col>{{ about.text }}</p><br><br><br></div></div><div class="col-md-1 col-lg-2"></div></div><!-- project --><div id=project class="row text-center"><br><br><h3>PROJECTS</h3><br><div class="col-md-1 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-10 col-lg-8"><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6" ng-repeat="project in main.projects"><div class=thumbnail><!-- <div class="project-card"> --><div class="col-xs-3 col-sm-3 col-md-4 col-lg-3"><br><a ng-href=#/projects/{{project.id}}><img style="padding: 0" class=pull-left ng-src="{{ project.logo }}" alt="{{ project.title }}"></a></div><div class="col-xs-9 col-sm-9 col-md-8 col-lg-9"><!-- <div class="col-lg-12 text-center"> --><div class="caption text-left"><h4><a ng-href=#/projects/{{project.id}}>{{ project.title }}</a></h4><a ng-href=#/projects/{{project.id}}><h5>{{ project.description }} <a ng-href=#/projects/{{project.id}}>...more</a></h5></a><span></span><h6 class="hidden-xs hidden-sm"><i class="fa fa-tags" aria-hidden=true></i> <span ng-bind-html=project.tagline></span></h6></div></div><div class="col-2 hidden-md hidden-lg"><div class="col-10 hidden-md hidden-lg"><h6><i class="fa fa-tags" aria-hidden=true></i> <span ng-bind-html=project.tagline></span></h6></div></div></div></div></div><!-- </div> --><div class="col-md-1 col-lg-2"></div></div><div id=contact class=row><br><br><br><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6 text-center"><h3 style="color: #fff" class=text-center>CONTACT NII</h3><h5>Feel free to contact me about business, or coding</h5><br></div><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6 text-center"><h5><i class="fa fa-user" aria-hidden=true></i> NII MANTE</h5><h5><i class="fa fa-location-arrow" aria-hidden=true></i> LOS ANGELES, CA</h5><h5><i class="fa fa-envelope" aria-hidden=true></i> <a href=mailto:nii@niimante.com>NII@NIIMANTE.COM</a></h5><br><br><br></div></div><div id=social class=row><!--  social icons --><div class="socials col-lg-4 col-md-4 col-sm-4 col-xs-4 text-center" ng-repeat="social in main.socials"><br><a href={{social.link}} target=_blank><i class="{{social.icon}} social-icon"></i></a><br><br></div></div></div>'),e.put("app/projects/projects.html","<div ng-include=projectsController.project.template></div>"),e.put("app/resume/resume.html",""),e.put("app/components/navbar/navbar.html",'<nav class="navbar navbar-default navbar-fixed-top wow slideInDown"><div class=container><div class=navbar-header><a id=brand class=navbar-brand ui-sref=home.masthead><b>NM</b> </a><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=#navbar aria-expanded=false aria-controls=navbar><span class=sr-only>Toggle navigation</span> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span></button></div><div class="collapse navbar-collapse" id=navbar><ul id=navbar-ul class="nav navbar-nav navbar-right"><!-- <li class="dropdown">\n          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">\n            Home <span class="caret"></span>\n          </a>\n          <ul class="dropdown-menu">\n            <li><a ng-href="#about" target="_self">About</a></li>\n            <li><a ng-href="#project" target="_self">Projects</a></li>\n            <li><a ng-href="#contact" target="_self">Contact</a></li>\n          </ul>\n        </li> --><li><a ui-sref=home.masthead ui-sref-active=active>Home</a></li><li><a ui-sref=home.about ui-sref-active=active>About</a></li><li><a ui-sref=home.project ui-sref-active=active>Projects</a></li><li><a ui-sref=home.contact ui-sref-active=active>Contact</a></li><!-- <li><a ng-href="#/blog">Blog</a></li> --><!-- <li><a ng-href="#/resume">Resume <i class="fa fa-download" aria-hidden="true"></i></a></li> --></ul></div></div></nav>'),e.put("app/model/projects/dedup.html",'<div class="jumbotron text-center dedup-jumbotron-bg"></div><div class="container-fluid wow fadeInUp"><div class="row text-center project-text-col"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h4 class="wow fadeInUp">{{projectsController.project.description}}</h4><br></div><div id=images class="row proxee-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h4 class=text-center>OVERVIEW</h4><p>Dedup is a python program I created as apart of my Information Retrieval class. The premise relates to search engines and web crawlers. During the class, 3 group members and I built a web crawler to grab images and webpages from the internet. Web crawlers are tools/programs that automatically grab/index content from the internet.</p><p>Essentially when web crawlers are indexing/crawling/downloading web pages on the internet there are times when the same data may be crawled (aka "looked at") repeatedly. Because of this you could end up downloading the same webpages, images or content; this can lead to unnecessary waste of memory or even worse, your crawler getting stuck (\'spider traps\'). Thus, deduplication is one technique used to get around these problems.</p><p>Feel free to check out the project on <a ng-href={{projectsController.project.url}} target=_blank>Github</a></p><h4 class=text-center>MY TASKS</h4><p>To accomplish deduplication I:<ul><li><b>PROGRAM DESIGN:</b> Came up with a pipeline for a command line application</li><li><b>ALGORITHM DESIGN</b> Came up with a way to \'featurize\' images to efficiently compare them</li></ul></p><p>The entire program was written in <b>Python</b>. I chose Python because it\'s got an awesome abstraction for multiprocess programming that makes it quick write highly parallel programs.</p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center proxee-text-col"><h4 class=text-center>PIPELINE</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="dedup-limit wow fadeIn" ng-src={{projectsController.project.figures.flowDiagram.medium}} alt="Dedup Flow Diagram"><h6><b>Figure 1:</b> The deduplication pipeline.</h6><br></div></div><br><div id=roommate class="row proxee-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the deduplication pipeline. This architecture allowed me to write a program that can deduplicate <b>~70000</b> images in about <b>10 seconds</b>.</p><br><h4 class=text-center><b>Techie Section:</b> What\'s happening behind the scenes?</h4><p><ul><li>Near De-duplication</li><li>Exact De-duplication</li><li>"MapReduce"</li></ul>It was extremely necessary to write a program that could deduplicate efficiently. No one wants to wait forever for a task such as this. To speed up the process, the python program:<ul><li><b>Split</b> Splits the data into M smaller segments</li><li><b>Workers</b> Creates M worker processes</li><li><b>Deduplicate</b> Tells each worker process to deduplicate it\'s smaller data chunk</li><li><b>Merge</b> Then merge the deduplicated images from the smaller sets</li></ul></p><p>The deduplication has two methodologies. One is \'exact\' deduplication, and the other is \'near\' deduplication. An exact duplicate set of images would have all the same exact pixels in the same exact positions. A nearly duplicate set of images could be the same images with slightly differing brightness, or slightly different pixel values.</p><h4>Exactly!</h4><p>Exact deduplication can be done fairly quickly, by simply \'hashing\' the image bytes and storing the hash/value pairs in a lookup table/dictionary. This ends up being extremely efficient, and you can iterate and deduplicate the data in one pass. Then add that to the fact that it\'s working on multiple cores/processes and you get a blazingly fast program.</p><h4>The end is Nigh... Near?</h4><p>Near deduplication is slightly different. In this case, you can\'t just simply \'hash\' the image. If the images are different in ANY way, and you hash them, the resultant hashes will be unbelievably different. That\'s a desired result of hashing, but it doesn\'t suit the problem we\'re trying to solve.</p><p>It would be nice if we could get similar hashes when the items were only slightly different. Turns out that some awesome people at Google came up with what\'s called <code>Simhash</code>. Essentially, it gives us the ability to reduce a huge piece of data (an image) to a small \'fingerprint\'! In addition to reducing the data, it also lets two pieces of data that are somewhat \'similar\' to have similar hashes (aka sim... hashes...). To generate the finger print I used features such as the image height, width, portions of the image bytes, etc.</p><p>I applied a similar high level logic to solving the near de-duplication problem. Instead of simply seeing if a hash value exists within the lookup table, I check to see if a \'nearly\' similar hash value exists within the lookup table. To speed this up even more, I use an index which sits on top of the data.</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/liltron.html",'<div class="jumbotron liltron-jumbotron text-center liltron-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h4 class="wow fadeInUp">{{projectsController.project.description}}</h4><h5 class="wow fadeInUp">{{projectsController.project.date}}</h5><br></div><div id=images class="row liltron-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h4 class=text-center>OVERVIEW</h4><p>Lil\' Tron is a project that two other students and myself built as apart of an Applied Natural Language Processing class. Our goal was to build a program which could generate cool rap lyrics on it\'s own! The app consisted of a front end web app, an artificially intelligent backend.</p><h4 class=text-center>WHAT I DID</h4><p>My role in this project was:<ul><li><b>FRONT END:</b> Create a <b>AngularJS</b> front end web app to display auto generated rap lyrics. Grab lyrics from other rap bot websites. Create a voting system to let people choose between our lyrics and competitor\'s lyrics.</li><li><b>BACK END:</b> Create a <b>NodeJS</b> & <b>Python</b> back end to run the Natural Language Processing algorithms my team and I built.</li></ul></p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h4 class=text-center>THE LOOK</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class=liltron-limit ng-src={{projectsController.project.figures.iphone.medium}} alt="Lil Tron iPhone"><h6><b>Figure 1:</b> The Lil Tron web app.</h6><br></div></div><br><div id=roommate class="row liltron-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> The app gives a simple interface. Users could press the \'Rhymify\' button and be presented with two sets of lyrics. Our generated lyrics, and lyrics from another website. They then had the choice of choosing their \'Favorite\' lyric.</p><br><h4 class=text-center><b>Techie Section:</b> What\'s happening behind the scenes?</h4><p><ul><li>AngularJS Web App (Me)</li><li>NodeJS Web Server (Me)</li><li>Python NLP Server (Team)</li><li>MongoDB Persistence (Me)</li></ul></p><br><h4 class=text-center>NLP Engine</h4><p>Let\'s start from the core. The NLP engine for generating Rap lyrics. To build this engine we needed three things:<ul><li>Language Model (Teammate 1 - Akshay)</li><li>Rhyming \'Goodness\' (Teammate 2 - Gandahli)</li><li>Thesaurus (Me)</li></ul>These three steps were implemented in <b>Python</b>. I\'ll first talk about how we accomplished each task. Then I\'ll talk about how each of the pieces worked together.</p><h5 class=text-center>N-GRAM</h5><p>Akshay crawled the lyrics of about 10000 rap songs, and generated an NGram language model. To put it briefly, an ngram language model stores phrases of words, and a probability score (from 0 - 1) for how likely it is to see those phrases. The \'N\' in Ngram says how many words per phrase you store. The NGram allows us to generate words based on previous words seen.</p><h5 class=text-center>RHYMING \'GOODNESS\'</h5><p>The next thing that is important, is the notion of rhyming. Generating sentences is cool, but for rap to sound good, we need rhyming words. To do this, Gandhali wrote a program to grab rhyming scores from a website; the website gives numerical scores for how well words rhyme with eachother. So given a word, we have a list of words that rhyme with it, as well as scores for how well each word rhymes.</p><h5 class=text-center>RELATED WORDS</h5><p>The last piece is to add some variability to the words the Ngram language model uses. The NGram generates words based on what it has seen already (it\'s vocabulary). The rhyming portion lets you know how well two words rhyme. But now you need add some spontaneity to the rhymes so they don\'t seem stale. This is where a thesaurus comes in. A thesaurus tells you the synonyms for a given word. To add more similar words to the one in our vocabulary, I created a program to grab words from thesaurus websites and store them in a JSON dictionary.</p><h5 class=text-center>PUT IT ALL TOGETHER</h5><p>Now that we have all the pieces, let\'s talk about how they fit together.<ol><li>Generate 18 words via the NGram language model. This will be a sentence (aka 2 bars)</li><li>Break the sentence into two pieces</li><li>Look at the last words of piece 1 and piece 2. We\'ll call them "word-1" and "word-2".</li><li>Use the thesaurus to find the related words/synonyms for "word-1" and "word-2". We\'ll call those "related-list-1" and "related-list-2", respectively. Also use part of speech tagging to make sure nouns match with nouns, adjectives with adjectives, etc.</li><li>Iterate through the combinations of words and find the pair that gives you the highest rhyme score</li><li>Put replace "word-1" and "word-2" with the pair of words you found!</li><li>Repeat this 3 times to get 16 bars!</li></ol></p><br><h5 class=text-center>SUMMARY</h5><p>Great so now that we\'ve discussed the NLP engine, let\'s talk about the rest of it; making an application out of this. The goal was to host this online so that peers within our class could try out our application. I was faced with the task of deploying this online.</p><p>I wrote an AngularJS web app to query the backend for rap lyrics. I also wrote the backend server(s) to handle requests. The backend was composed of three pieces:<ul><li>NodeJS webserver</li><li>Python NLP Server</li><li>MongoDB Persistence</li></ul></p><p>The NodeJS webserver handled requests from the AngularJS web app. Once the user pressed the \'rhymify\' button on the webapp, the NodeJS server queries the python NLP engine server for lyrics. Once the lyrics are generated, the NodeJS server returns the lyrics back to the web app. All of this was stored on Amazon Web Services.</p><p>f To keep the app running quickly, I stored the language model, thesaurus and rhyme dictionary in memory. This ended up being a little tricky at first because the files were larger then the provided ram (> 4 GB). To get around this I increased the virtual memory (via swap file) and things worked like a charm! It takes about <b>10 seconds</b> from the time \'rhymify\' is clicked to return lyrics to the user! We could definitely have sped this up via multiprocessing or distributing the load, but we also had to implement this within a couple of weeks. Not bad if you ask me!</p><p>The last step happens when a user chooses a \'favorite\' lyric. We store the lyric they chose as well as which site generated the lyric (ours, or the competitor). We give no indication which lyric is ours or the competitors. This was our validation test; it let us see if we were generating good lyrics. After leaving the voting open for a few days, our lyrics <b>outperformed</b> the competitor!</p></div><div class="col-md-2 col-lg-2"></div></div></div>'),
e.put("app/model/projects/mega.html",'<div class="jumbotron mega-jumbotron text-center mega-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h4 class="wow fadeInUp">{{projectsController.project.description}}</h4><h5 class="wow fadeInUp">{{projectsController.project.date}}</h5></div><div id=overview class="row proxee-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h4 class=text-center>OVERVIEW</h4><p><a ng-href={{projectsController.project.url}} target=_blank>The Code</a></p><p>When I started getting into web design, I built a website for a student organization called MEGA (Minority Engineering Graduate Association) at USC. At the time, I was the president of the organization, and I was interested in revamping the website.</p><p>I built the mockup website using <b>AngularJS</b>, and <b>Twitter Bootstrap</b>. To make development efficient, I used a command line tools called <code>Grunt</code> and <code>Karma</code>. What do they do?<ul><li><b>Grunt:</b> A task runner that allows you to automate things. Really useful for \'minifying\' and \'uglifying\' your css and js files. Basically that boils down to shrinking your code so that it takes your web browser less time to download and render your website!</li><li><b>Karma:</b> An awesome test harness that allows you to test your JS</li></ul></p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center proxee-text-col"><h4 class=text-center>THE RESULT</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="mega-limit wow fadeIn" ng-src={{projectsController.project.figures.fullWeb.medium}} alt=""><h6><b>Figure 1:</b> The main page for the MEGA website.</h6><br></div></div><br><div id=roommate class="row proxee-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the final result for the MEGA website. I really wanted to make a simple page that highlighted the most important things about MEGA. The first thing a person would see when they went to the website would be a <code>Carousel</code> aka an <code>Image Slider</code>. The image slider showcased us at social events, community service events and also at professional events like the NSBE national convention. Additionally, I leveraged Twitter Bootstrap to make the website responsive, so that it would look nice on a screen of any size!</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/olts.html",'<div class="jumbotron olts-jumbotron text-center olts-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center olts-text-col"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}} - PhD Thesis Work</h3><h4>{{projectsController.project.date}}</h4></div><div id=images class="row olts-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h4 class=text-center>OVERVIEW</h4><p>Those interested in code check it out on <a target=_blank href={{projectsController.project.url}}>Github</a>!</p><p>OLTS is a <b>C++</b> based system that I built as apart of my PhD thesis. OLTS stands for Object Tracking and Localization System. It is one module of a tool called the "Wearable Visual Aid" (see Figure 1) to help the blind do daily tasks. The system is composed of three pieces:<ul><li>Head Mounted Camera</li><li>Computer</li><li>Headphones/Vibration Motors</li></ul></p><p>First a camera fed visual information to the computer. The computer contains algorithms that can track an object real time. The objects in this case are grocery store items. While tracking the object, the computer generates speech or vibrotactile feedback based on the objects position. So essentially, you get a computer telling the blind person how to reach and grasp for grocery store items!</p><h4 class=text-center>WHAT I DID</h4><p>My goal was to build a proof of concept system to be tested with visually impaired people. To do this I did a few things:<ul><li><b>HUMAN SUBJECTS TESTING:</b> It\'s really awesome building stuff for people. What\'s even cooler is letting people try what you\'ve built. I tested the system I built with 13 visually impaired subjects</li><li><b>SOFTWARE DESIGN:</b> I needed to figure out an efficient way to make the computer vision modules work with the speech/vibration feedback modules.</li></ul></p><p>After testing with the 13 visually impaired subjects, we determined that subjects were able to find the correct item (out of 3-5 items) within <b>20 seconds</b>.</p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h4 class=lead>THE WEARABLE VISUAL AID</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="olts-limit wow fadeIn" ng-src={{projectsController.project.figures.flowchart.original}} alt=""><h6><b>Figure 1:</b> The Wearable Visual Aid. A proof of concept closed loop grocery shopping assistant for visually impaired people.</h6><br></div></div><br><div id=olts-section2 class="row olts-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the Wearable Visual Aid (WVA). It\'s a closed loop system that:<ul><li>Lets a visually impaired person choose a grocery store item via smartphone</li><li>Uses computer vision based object recognition to find items in video</li><li>Generate real time feedback based on the object\'s position</li></ul></p><p>The WVA was an interdisciplinary project. My system, OLTS was one piece of the WVA system.</p><br><h4 class=text-center><b>Techie Section: Under the Hood</b></h4><p>The OLTS is a multithreaded OpenCV C++ program. It\'s run as a command line application. One of the libraries I\'m using is Windows only and proprietary, so it only runs on Windows! I\'m using Boost threading library to make the program run in real time. The program gives an option for running in speech mode, or vibrotactile mode. In both cases, the structure of the program is as follows:<ul><li><b>Thread 0:</b> Main Thread</li><li><b>Thread 1:</b> Vision Thread</li><li><b>Thread 2:</b> Feedback Thread</li><li><b>Thread 3:</b> Data Logging Thread</li></ul></p><p><b>Main Thread: </b>The main thread is responsible for handling configuration options like which feedback mode, or even parameters for the feedback portion of the program. At the start of the program, the main thread starts a GUI to select a region within an image. This gives the researcher (me) a chance to draw a box around the grocery item of interest. Once I\'ve done that, then I let the program run autonomously and speak to the visually impaired person. The main thread is also responsible for creating the vision, feedback and data logging threads.</p><p><b>Vision Thread: </b>The vision thread is responsible for grabbing frames from the camera and also taking care of doing computer vision processing. Specifically, the computer vision algorithm is an Object Tracking program called the <a href=http://iris.usc.edu/people/thangdin/research.html target=_blank>Context Tracker</a>. The tracking algorithm gives us real time positional updates of the selected item and stores this in a shared variable.</p><p><b>Feedback Thread: </b>The feedback thread is responsible reading from the position shared variable. Upon reading from the shared variable, it generates vibration or speech based on the position in the field of view. Essentially, we split the camera frame into 9 distinct regions. I call it the "Sensory Map" (see figure 2). If an object falls in a region, then the feedback thread acts accordingly. The words generated by the computer are simple commands like "Up", "Down", etc. The vibrations come from 4 motors (M1 - M4). We place these motors on the headphones as well.</p><p><b>Data Logging Thread: </b>The data logging thread records what the camera is seeing as well as the tracking path of the object over time.</p><br></div><div class="col-md-2 col-lg-2"></div></div><div class="row text-center olts-text-col"><h4 class=lead>THE SENSORY MAP</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="olts-limit text-center wow fadeIn" ng-src={{projectsController.project.figures.sensory.original}} alt="Sensory Map"><h6><b>Figure 2:</b> The Sensory Map. This represents the camera\'s field of view. If an item falls in a certain region/box then the computer speaks or vibrates the command.</h6><br></div></div></div>'),e.put("app/model/projects/proxee.html",'<div class="jumbotron project-jumbotron text-center proxee-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center"><h1 class="wow fadeInDown">PROXEE</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}}</h3></div><div id=images class="row proxee-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h4 class=text-center>OVERVIEW</h4><p>Proxee is an iOS app I started developing during the summer of 2015. A friend and I had an idea about making it easier for people to not only find great places to live, but to also find great people to live with.</p><br><h4 class=text-center>WHAT I DID</h4><p>My role in this project was:<ul><li><b>DESIGN:</b> Coming up with a slick design and UX for the app</li><li><b>FRONT END:</b> Writing the app code and and bringing the design to life</li><li><b>BACK END:</b> I wired up a simple backend on Parse to store user & apartment data</li></ul></p><p>To write the app, I picked up <b>Swift</b>. I hadn\'t programmed in Swift before, but I had experience in Objective-C prior to that. It was a really smooth transition from Obj-C to Swift. (I think Apple did an amazing job btw with the language!) We never launched the app to the app store, but it was a great experience writing the app. I\'m looking forward to the day when I completely launch my first app!</p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h4 class=text-center>THE LOOK</h4><div class="col-xs-12 col-sm-12 col-md-6 col-lg-6"><img class="project-limit wow fadeIn" ng-src={{projectsController.project.figures.listing.large}} alt=""><h6><b>Figure 1:</b> The apartment listing feed.</h6><br></div><div class="col-xs-12 col-sm-12 col-md-6 col-lg-6"><img class="project-limit wow fadeIn" ng-src={{projectsController.project.figures.roommate.large}} alt=""><h6><b>Figure 2:</b> Like Tinder... for roommate matching</h6><br></div></div><br><div id=roommate class="row proxee-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the apartment listing feed. The goal for this page was to make the focus all on the actual listings themselves. I took inspiration from Pinterest, and chose what\'s called a \'waterfall\' layout to let each apartment listing flow into the next. I was really interested in minimizing the details (price, bed/bath, etc.) just enough. Just enough so that they\'re legible, but not to large that they take focus off the real content.</p><br><h4 class=text-center><b>Techie Section:</b> What\'s happening behind the scenes?</h4><ul><li>Smooth Scrolling</li><li>MVVM - Model - View - View Model</li></ul><p>To make scrolling really fast and responsive I needed to asynchronously load images. Any task that required network IO was off loaded to background tasks. Definitely a standard practice to do this, but I think it\'s extremely important to note that fact.</p><p>To make it easy to manipulate apartment listing data and also create a separation of concerns between data and presentation, I adopted a design paradigm called MVVM (Model - View - View Model). I chose this instead of the standard MVC (Model - View - Controller) to avoid really massive view controllers. Essentially, the MVC paradigm often tends to lead to Controllers that do EVERYTHING. Network calls, loading data into views, etc. MVVM is a nice way to separate data manipulation and view logic. Additionally, it allows you to reuse a lot of the same components extremely easily accross multiple views.</p><p><b>Figure 2</b> shows the roommate matching portion of the app. My logic for this page was to use a layout that most of our target demographic would be familiar with. People in their 20\'s are fairly aware of dating apps, so a Tinder like interface is both familiar and easy to use for that demo. \'Swipe Right\' if you think you could live with that person and swipe... yeah you get the idea.</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/visor.html",'<div class="jumbotron visor-jumbotron text-center visor-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center visor-text-col"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}}</h3><h4>{{projectsController.project.date}}</h4></div><div id=images class="row visor-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h4 class=text-center>OVERVIEW</h4><p>VISOR.AI is a deep learning module that I built as apart of my PhD Thesis. VISOR is a play on the words <b>O</b>bject <b>R</b>ecognition for the <b>VIS</b>ually impaired. The specific purpose of VISOR.AI was to recognize grocery store items in images. This module fits into a larger system that aims to help visually impaired people with daily tasks. In this case that daily task is grocery shopping.</p><br><h4 class=text-center>WHAT I DID</h4><p>My goal for this project is to accomplish object recognition in real time. To do this I\'m doing the following:<ul><li><b>DATA PIPELINE:</b> One of the biggest pieces of a good neural net is the data. I created a pipeline to grab grocery store images and preprocess them.</li><li><b>NETWORK DESIGN:</b> I\'m coming up with a compact deep learning net</li><li><b>VISUALIZATION:</b> It\'s important to see how well your network is doing, as well as what your features look like. I\'m also writing code to accomplish this</li></ul></p><p></p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h4 class=text-center>THE NETWORK</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="visor-limit wow fadeIn" ng-src={{projectsController.project.figures.network.medium}} alt=""><h6><b>Figure 1:</b> Version 1 of the VISOR.AI neural network.</h6><br></div></div><br><div id=roommate class="row visor-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows a network diagram for the VISOR.AI network. The system takes in a 50x50 pixel image, and ultimately gives a guess for category of the item. Right now there are 8000 items that the network has been trained on. An example of an item could be a box of \'Cheerios\'. These 8000 items all fall into some category. So \'Cheerios\' would fall into a category such as \'Food/Cereal\'! At the moment, the network processes an image in <b>9 milliseconds</b>. The overall accuracy of the system is <b>~43%</b>.</p><br><h4 class=text-center><b>Current Work</b></h4><p>This project is currently in progress. Currently, I am:<ul><li>Gathering more grocery images to refine the data used for training.</li><li>Trying different image resolutions greater than 50x50 pixels</li><li>Refining parameters such as types & number of layers</li><li>Refining hyperparameters like learning rate, epochs, momentum</li></ul></p><p>I will update this page as I make progress. Stay tuned!</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/wva.html",'<div class="jumbotron wva-jumbotron text-center wva-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center wva-text-col"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h6 class="wow fadeInUp">{{projectsController.project.description}} - PhD Thesis Work</h6><h4>{{projectsController.project.date}}</h4><br></div><div class="row wva-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><p><b>TL;DR</b> <i>Problem: </i>It\'s difficult for blind people to do daily tasks like shopping. <i>Proposed Solution:</i> I built a system that uses cameras and computer vision algorithms to find grocery items in real time (Figure 1, 2). I also built an iOS app (Figure 3) to allow a person (blind or blindfolded) to choose items from a list, so that they can eventually grab items in front of them.</p><h4 class=text-center>OVERVIEW</h4><!-- <p>\n        Those interested in code check it out on <a target="_blank" href="{{projectsController.project.url}}">Github</a>!\n      </p> --><p>WVA 2.0 is a <b>C++</b> based computer vision system that I built as apart of my PhD thesis. WVA 2.0 is the second version of the "Wearable Visual Aid". The first version of the system (WVA 1.0) was built by myself and a group of computer scientists, biomedical, and electrical engineers.</p><p>Briefly, the goal of the WVA 1.0 was to allow a visually impaired person to find grocery items using a wearable system. The system is composed of 4 pieces:<ul><li>iOS Controller Application</li><li>Head Mounted Camera</li><li>Computer + Algorithms</li><li>Headphones/Vibration Motors</li></ul></p><p>Figure 2 shows a flow diagram for how each of these pieces fit together.</p><p>First a visually impaired person clicks an item from a pre-populated list of grocery items within the iOS app (e.g. <i>Pasta Roni</i>). Once they click the item on the phone, the backend computer requests video from the head mounted camera. The computer then uses text/object recognition algorithms to see if the item is in front of the visually impaired person. If the computer finds the item, it then uses a tracking algorithm to track the item in real time.</p><p>While tracking the object, the computer generates speech or vibrotactile feedback based on the objects position. The feedback tells the blind person to turn their head "Left", "Up", "Down and Left", etc. so that they can eventually center the item in the camera\'s field of view. Once they\'ve centralized the object, they can reach directly where the camera is pointing. Essentially, you get a computer telling the blind person how to reach and grasp for grocery store items (Figure 1)!</p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center wva-text-col"><h4>REACH AND GRASP</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="wva-limit text-center wow fadeIn" ng-src={{projectsController.project.figures.demo.original}} alt="WVA 2.0 Demo"><h6><b>Figure 1:</b> The final result. In this image, a <b>blind-folded</b> sighted subject uses the smartphone app to choose an item (<i>e.g.</i> Rice Krispies), and the WVA 2.0 finds the item in the field of view. The algorithm runs extremely quickly despite the heavy processing going on (12 frames per second).</h6><br></div></div><div id=images class="row wva-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><br><h4 class=text-center>WHAT I DID</h4><p>My goal was to build a better version of the WVA 1.0. The result was the WVA 2.0. To do this I did a few things:<ul><li><b>OBJECT RECOGNITION</b> I needed to create a recognition system that could do object recognition in real time. Our previous recognition system could do recognition in about 9 seconds. I wanted mine to do it in under a second.</li><li><b>TEXT RECOGNITION</b> Our previous system didn\'t incorporate text recognition algorithms for finding objects. I felt that text recognition could aid the system in finding objects in real time.</li><li><b>USEFUL FEEDBACK</b> The previous system wasn\'t very informative about ongoing tasks. I added more useful feedback and accessibility features to make the experience more pleasant.</li><li><b>SOFTWARE DESIGN:</b> The final part was creating an end to end application. I had to make design decisions to make all the modules work together.</li></ul></p><p>After completing the prototype I was able to test it with a few blind-folded sighted subjects. The WVA 2.0 system that I built could find items within <b>1-2 seconds</b>. To put it in perspective, our <i>previous</i> system took at least 9 seconds to find objects.</p><br></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><h4>WVA 2.0</h4><img class="wva-limit wow fadeIn" ng-src={{projectsController.project.figures.flowChart.medium}} alt="WVA Flowchart"><h6><b>Figure 2:</b> The Wearable Visual Aid 2.0. A proof of concept fully closed loop grocery shopping assistant for visually impaired people.</h6><br></div></div><br><div id=wva-section2 class="row wva-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 2</b> shows flow diagram for the Wearable Visual Aid (WVA) 2.0. It\'s a closed loop system that:<ul><li>Lets a visually impaired person choose a grocery store item via smartphone</li><li>Uses computer vision based text/object recognition to find items in video</li><li>Generate real time feedback based on the object\'s position</li></ul></p><p></p><br><h4 class=text-center><b>Techie Section: Under the Hood</b></h4><p>The WVA 2.0 consists of a:<ul><li><b>Front End Interface</b>: <i>VoiceOver based <b>iOS</b> app</i></li><li><b>Backend/Computer Vision Algorithms</b>: Multithreaded <b>OpenCV 3.1</b> based <b>C++11</b> program</li><li><b>Feedback Interface</b>: Speech Synthesis + headphones for voice feedback</li></ul></p><p><b>Front End Interface: </b>The front end application is an iOS 9 application. It uses an iOS feature called <a href=http://www.apple.com/accessibility/ios/voiceover/ target=_blank>VoiceOver</a> to tell the visually impaired person what they\'re clicking on the screen. The app communicates with the computer vision algorithms via User Datagram Protocol (UDP).</p></div><div class="col-md-2 col-lg-2"></div></div><div class="row wva-text-col"><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12 text-center"><h4 class=text-center>THE iOS APP</h4><img class="wva-limit wow fadeIn text-center" ng-src={{projectsController.project.figures.iOS.medium}} alt="iOS Accessibility App"><h6><b>Figure 3:</b> The iOS controller application. The blind individual can click items on the screen. The app uses <a href=http://www.apple.com/accessibility/ios/voiceover/ target=_blank>VoiceOver</a> to talk back to the user whenever a button is clicked.</h6><br></div></div><div id=wva-section3 class="row wva-text-col"><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Computer Vision Thread(s): </b>There are three computer vision algorithms running on the Backend. The backend is simply an OS X based laptop, but in future iterations the algorithms could directly be on the phone itself. Any time a request for recognition is made, these algorithms are started as asynchronous threads to keep the program responsive (done via <a href=https://solarianprogrammer.com/2012/10/17/cpp-11-async-tutorial/ target=_blank>C++11 async\'s</a>).</p><p>1. The first vision algorithm uses <i>Text Recognition</i> to find text within the image. If any text within the image matches text from the list of items, then the algorithm stores the location of the recognized text, as well as the text guess. The text recognition is a two phase process. First text boxes are detected via Class Specific Extremal Region detection (<a href=http://docs.opencv.org/3.1.0/da/d56/group__text__detect.html target=_blank>Neumann 2012</a>); if any text boxes are found, they are then passed through a Neural Network (Coates 2011) based text recognizer.</p><p>2. The next vision algorithm uses <i>Object Recognition</i> to find the location of an item within the image. The purpose of this step is to act as a redundancy for boosting overall recognition. The logic being that if both the object recognition and text recognition agree on location and text guessed, then it\'s very likely that the object exists at that location. This object recognition algorithm is based on my Neural Network (<a href=http://www.niimante.com/#/projects/visor target=_blank>visor.AI</a>). The accuracy of this algorithm is low, so as a result the WVA 2.0 algorithm mainly relies on text recognition.</p><p>3. The final vision algorithm uses <i>Object Tracking</i> to track the item in real time over a live video stream. The object tracking algorithm is called the <a href=http://iris.usc.edu/people/thangdin/research.html target=_blank>Context Tracker</a> (Dinh 2011); it\'s based off of the famous TLD tracking algorithm. The tracking algorithm gives us real time positional updates of the recognized item.</p><p><b>Feedback Thread: </b>The feedback thread is responsible generating speech based on the position of the item within the field of view. This thread is based on the first bit of work (<a href=http://www.niimante.com/#/projects/olts target=_blank>OLTS</a>) I did for my PhD. Essentially, we split the camera frame into 9 distinct regions. I call it the "Sensory Map" (see Figure 4).</p><p>If an object\'s centroid is within a region, then the feedback thread acts accordingly. The words generated by the computer are simple commands like "Up", "Down", etc. The vibrations come from 4 motors (M1 - M4). We place these motors on the headphones as well.</p><p>The feedback thread also generates useful information such as what item has been found (<i>"Looks like I found Pasta Roni"</i>) or if no items are present (<i>"I didn\'t see any items from your list"</i>).</p><br></div><div class="col-md-2 col-lg-2"></div></div><!-- End row --><div class="row text-center wva-text-col"><h4>THE SENSORY MAP</h4><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="wva-limit wow fadeIn text-center" ng-src={{projectsController.project.figures.sensory.original}} alt="Sensory Map"><h6><b>Figure 4:</b> The Sensory Map. This represents the camera\'s field of view. If an item falls in a certain region/box then the computer speaks or vibrates the command.</h6><br></div></div></div>')}]);
//# sourceMappingURL=../maps/scripts/app-1e82beaa66.js.map
