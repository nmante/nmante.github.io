!function(){"use strict";angular.module("nii-website",["ngAnimate","ngCookies","ngTouch","ngSanitize","ngMessages","ngAria","ngResource","ui.router","toastr"])}(),function(){"use strict";function e(){function e(){return t}var t=[{title:"AngularJS",url:"https://angularjs.org/",description:"HTML enhanced for web apps!",logo:"angular.png"},{title:"BrowserSync",url:"http://browsersync.io/",description:"Time-saving synchronised browser testing.",logo:"browsersync.png"},{title:"GulpJS",url:"http://gulpjs.com/",description:"The streaming build system.",logo:"gulp.png"},{title:"Jasmine",url:"http://jasmine.github.io/",description:"Behavior-Driven JavaScript.",logo:"jasmine.png"},{title:"Karma",url:"http://karma-runner.github.io/",description:"Spectacular Test Runner for JavaScript.",logo:"karma.png"},{title:"Protractor",url:"https://github.com/angular/protractor",description:"End to end test framework for AngularJS applications built on top of WebDriverJS.",logo:"protractor.png"},{title:"Bootstrap",url:"http://getbootstrap.com/",description:"Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.",logo:"bootstrap.png"}];this.getTec=e}angular.module("nii-website").service("webDevTec",e)}(),function(){"use strict";function e(){function e(){return t}var t=[{title:"PROXEE",tags:["iOS","mobile"],date:"June 2015",description:"An iOS app for helping people find apartments and roommates",url:"",id:"proxee",logo:"proxee.png",template:"app/model/projects/proxee.html"},{title:"DEDUP",date:"Fall 2015",tags:["Python","Algorithms"],description:"A multiprocess python program for de-duplicating images",url:"https://github.com/nmante/image_deduplication",id:"dedup",logo:"dedup-architecture-preview.tiff",template:"app/model/projects/dedup.html"},{title:"MEGA",date:"November 2013",tags:["HTML","CSS","JavaScript","Web"],description:"A mockup website for the Minority Engineering Graduate Association",url:"https://github.com/nmante/MEGA_Website",id:"mega",logo:"mega-web.tiff",template:"app/model/projects/mega.html"},{title:"LIL TRON",date:"April 2015",tags:["AngularJS","NodeJS","Python","AWS"],description:"An artificially intelligent rap lyric bot",url:"",id:"liltron",logo:"liltron-iphone.tiff",template:"app/model/projects/liltron.html"},{title:"OLTS",date:"2012 - 2015",tags:["C++","Computer Vision","Algorithms","PhD Thesis"],description:"A real time object tracking program for the visually impaired",url:"https://github.com/nmante/olts",id:"olts",logo:"terminal_icon.png",template:"app/model/projects/olts.html"},{title:"VISOR.AI",date:"Spring 2016",tags:["Deep Learning","Algorithms","Computer Vision","Lua","Torch","PhD Thesis"],description:"A deep learning program for recognizing grocery store items in real time",url:"",id:"visor",logo:"neural-net.png",template:"app/model/projects/visor.html"}];this.getProjects=e}angular.module("nii-website").service("projectService",e)}(),function(){"use strict";angular.module("nii-website").directive("mainNavbar",function(){function e(e,t){var o=this;o.url=t.absUrl(),o.relativeDate=e(o.creationDate).fromNow(),angular.element("#navbar a").click(function(){angular.element("#navbar").collapse("hide")})}e.$inject=["moment","$location"];var t={restrict:"E",templateUrl:"app/components/navbar/navbar.html",scope:{creationDate:"="},controller:e,controllerAs:"vm",bindToController:!0};return t})}(),function(){"use strict";function e(e){function t(t,o,a,i){var s,l=e(o[0],{typeSpeed:40,deleteSpeed:40,pauseDelay:800,loop:!0,postfix:" "});o.addClass("acme-malarkey"),angular.forEach(t.extraValues,function(e){l.type(e).pause()["delete"]()}),s=t.$watch("vm.contributors",function(){angular.forEach(i.contributors,function(e){l.type(e.login).pause()["delete"]()})}),t.$on("$destroy",function(){s()})}function o(e,t){function o(){return a().then(function(){e.info("Activated Contributors View")})}function a(){return t.getContributors(10).then(function(e){return i.contributors=e,i.contributors})}var i=this;i.contributors=[],o()}o.$inject=["$log","githubContributor"];var a={restrict:"E",scope:{extraValues:"="},template:"&nbsp;",link:t,controller:o,controllerAs:"vm"};return a}e.$inject=["malarkey"],angular.module("nii-website").directive("acmeMalarkey",e)}(),function(){"use strict";function e(e,t){function o(o){function i(e){return e.data}function s(t){e.error("XHR Failed for getContributors.\n"+angular.toJson(t.data,!0))}return o||(o=30),t.get(a+"/contributors?per_page="+o).then(i)["catch"](s)}var a="https://api.github.com/repos/Swiip/generator-gulp-angular",i={apiHost:a,getContributors:o};return i}e.$inject=["$log","$http"],angular.module("nii-website").factory("githubContributor",e)}(),function(){"use strict";function e(){}angular.module("nii-website").service("blogService",e)}(),function(){"use strict";function e(){}angular.module("nii-website").controller("ResumeController",e)}(),function(){"use strict";function e(e,t,o){for(var a=this,i=o.getProjects(),s=0;s<i.length;s++){var l=i[s];l.id.toLowerCase()==t.projectId.toLowerCase()&&(a.project=l)}}e.$inject=["$timeout","$stateParams","projectService"],angular.module("nii-website").controller("ProjectsController",e)}(),function(){"use strict";function e(e,t,o,a){var i=this;i.abouts=[{img:"Multiple-Devices-50.png",alt:"mobile-web",title:"FRONT END",text:"I love spending my spare time building iOS apps and websites."},{img:"Cloud-50.png",alt:"algos",title:"BACK END",text:"I've written NodeJS, Python, or even deep learning and computer vision programs."},{img:"Airport-50.png",alt:"travel",title:"TRAVELER",text:"Traveling is a passion of mine! I've visited Europe, Africa, South America and I hope to add more to the list soon."},{img:"Football-50.png",alt:"sports",title:"SPORTS",text:"I love playing soccer and basketball. I was even fortunate enough to go to the Brazil World Cup!"}],i.projects=t.getProjects();for(var s=0;s<i.projects.length;s++)for(var l="",r=i.projects[s].tags,n=0;n<r.length;n++)l+="<a ng-src=''>"+r[n]+"</a>",n!=r.length-1&&(l+=", "),i.projects[s].tagline=l;i.socials=[{name:"Github",id:"social-icon",link:"http://www.github.com/nmante",icon:"fa fa-github"},{name:"Twitter",id:"social-icon",link:"http://www.twitter.com/niimante",icon:"fa fa-twitter"},{name:"LinkedIn","class":"social-icon",link:"https://www.linkedin.com/pub/nii-mante/45/a27/6a4",icon:"fa fa-linkedin"}],angular.element('a[target="_self"]').click(function(e){if("/"===o.url()){e.preventDefault();var t=angular.element(this).attr("ng-href");angular.element("html,body").animate({scrollTop:angular.element(t).offset().top},"slow")}})}e.$inject=["$timeout","projectService","$location","$stateParams"],angular.module("nii-website").controller("MainController",e)}(),function(){"use strict";function e(e,t,o,a){}e.$inject=["$timeout","blogService","$location","$stateParams"],angular.module("nii-website").controller("BlogController",e)}(),function(){"use strict";function e(e){e.debug("runBlock end")}e.$inject=["$log"],angular.module("nii-website").run(e)}(),function(){"use strict";function e(e,t){e.state("home",{url:"/",templateUrl:"app/main/main.html",controller:"MainController",controllerAs:"main"}).state("projects",{url:"/projects/:projectId",templateUrl:"app/projects/projects.html",controller:"ProjectsController",controllerAs:"projectsController"}).state("blog",{url:"/blog",templateUrl:"app/blog/blog.html",controller:"BlogController",controllerAs:"blogsController"}).state("resume",{url:"/resume",templateUrl:"app/resume/resume.html",controller:"ResumeController",controllerAs:"resumeController"}),t.otherwise("/")}e.$inject=["$stateProvider","$urlRouterProvider"],angular.module("nii-website").config(e)}(),function(){"use strict";angular.module("nii-website").constant("malarkey",malarkey).constant("moment",moment)}(),function(){"use strict";function e(e,t,o){e.debugEnabled(!0),t.allowHtml=!0,t.timeOut=3e3,t.positionClass="toast-top-right",t.preventDuplicates=!0,t.progressBar=!0}e.$inject=["$logProvider","toastrConfig","$locationProvider"],angular.module("nii-website").config(e)}(),angular.module("nii-website").run(["$templateCache",function(e){e.put("app/blog/blog.html",""),e.put("app/main/main.html",'<div id=masthead class="jumbotron vertical-center text-center"><div class=container><h1 class="wow fadeInDown">NII MANTE</h1><p class="lead wow fadeInDown">Full Stack Software Engineer</p><h4 class="socials wow fadeInUp"><span ng-repeat="social in main.socials"><a href={{social.link}} target=_blank><i id={{social.name}} class={{social.icon}}></i></a></span></h4></div></div><div class=container-fluid><div id=about class="row text-center"><br><br><h3>ABOUT NII</h3><div class="col-sm-2 col-xs-1 col-md-3 col-lg-3"></div><div class="col-sm-8 col-xs-10 col-md-6 col-lg-6 text-justify"><br><p>I\'m Nii. I\'m a PhD candidate at the University of Southern California. I\'ll be graduating from there in 2016 with an MS in Computer Science and PhD in Biomedical Engineering. I\'ve been researching and building computer vision systems to help the blind. I graduated from the University of Maryland in 2010 with a BS in Bioengineering.</p><br><br><br></div><div class="col-sm-2 col-xs-1 col-md-3 col-lg-3"></div><br><br><br><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6 text-center wow slideInUp" ng-repeat="about in main.abouts"><img ng-src=assets/images/{{about.img}} alt="{{ about.alt }}"><br><br><p class=lead>{{ about.title }}</p><p class=about-col>{{ about.text }}</p><br><br><br></div></div><div id=project class="row text-center"><br><br><h3>PROJECTS</h3><br><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6" ng-repeat="project in main.projects"><div class=thumbnail><div class="col-xs-3 col-sm-3 col-md-3 col-lg-3"><br><a ng-href=#/projects/{{project.id}}><img style="padding: 0" class=pull-left ng-src="assets/images/{{ project.logo }}" alt="{{ project.title }}"></a></div><div class="col-xs-9 col-sm-9 col-md-9 col-lg-9 text-left"><!-- <div class="caption text-left"> --> <a ng-href=#/projects/{{project.id}}><h4 class=lead>{{ project.title }}</h4></a><p>{{ project.description }} <a ng-href=#/projects/{{project.id}}>...more</a></p><span></span><p><i class="fa fa-tags" aria-hidden=true></i> <span ng-bind-html=project.tagline></span></p><!-- </div> --></div></div></div></div><div id=contact class=row><br><br><br><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6 text-center"><h3 style="color: #fff" class=text-center>CONTACT NII</h3><p class=lead>Feel free to contact me about business, or coding</p><br></div><div class="col-xs-12 col-sm-6 col-md-6 col-lg-6 text-center"><p class=lead><i class="fa fa-user" aria-hidden=true></i> NII MANTE</p><p class=lead><i class="fa fa-location-arrow" aria-hidden=true></i> LOS ANGELES, CA</p><p class=lead><i class="fa fa-envelope" aria-hidden=true></i> <a href=mailto:nii@niimante.com>NII@NIIMANTE.COM</a></p><br><br><br></div></div><div id=social class=row><!--  social icons --><div class="socials col-lg-4 col-md-4 col-sm-4 col-xs-4 text-center" ng-repeat="social in main.socials"><br><a href={{social.link}} target=_blank><i class="{{social.icon}} social-icon"></i></a><br><br></div></div></div>'),e.put("app/projects/projects.html","<div ng-include=projectsController.project.template></div>"),e.put("app/resume/resume.html",""),e.put("app/components/navbar/navbar.html",'<nav class="navbar navbar-default navbar-fixed-top wow slideInDown"><div class=container><div class=navbar-header><a class=navbar-brand href=/ >NM </a><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=#navbar aria-expanded=false aria-controls=navbar><span class=sr-only>Toggle navigation</span> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span></button></div><div class="collapse navbar-collapse" id=navbar><ul class="nav navbar-nav navbar-right"><!-- <li class="dropdown">\n          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">\n            Home <span class="caret"></span>\n          </a>\n          <ul class="dropdown-menu">\n            <li><a ng-href="#about" target="_self">About</a></li>\n            <li><a ng-href="#project" target="_self">Projects</a></li>\n            <li><a ng-href="#contact" target="_self">Contact</a></li>\n          </ul>\n        </li> --><li><a ng-href=/ >Home</a></li><li><a ng-href=#about target=_self>About</a></li><li><a ng-href=#project target=_self>Projects</a></li><li><a ng-href=#contact target=_self>Contact</a></li><!-- <li><a ng-href="#/blog">Blog</a></li> --><!-- <li><a ng-href="#/resume">Resume <i class="fa fa-download" aria-hidden="true"></i></a></li> --></ul></div></div></nav>'),e.put("app/model/projects/dedup.html",'<div class="jumbotron text-center dedup-jumbotron-bg"></div><div class="container-fluid wow fadeInUp"><div class="row text-center project-text-col"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}}</h3></div><div id=images class="row proxee-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h1 class="lead text-center">OVERVIEW</h1><p>Dedup is a python program I created as apart of my Information Retrieval class. The premise relates to search engines and web crawlers. During the class, 3 group members and I built a web crawler to grab images and webpages from the internet. Web crawlers are tools/programs that automatically grab/index content from the internet.</p><p>Essentially when web crawlers are indexing/crawling/downloading web pages on the internet there are times when the same data may be crawled (aka "looked at") repeatedly. Because of this you could end up downloading the same webpages, images or content; this can lead to unnecessary waste of memory or even worse, your crawler getting stuck (\'spider traps\'). Thus, deduplication is one technique used to get around these problems.</p><p>Feel free to check out the project on <a ng-href={{projectsController.project.url}} target=_blank>Github</a></p><h1 class=lead>MY TASKS</h1><p>To accomplish deduplication I:<ul><li><b>PROGRAM DESIGN:</b> Came up with a pipeline for a command line application</li><li><b>ALGORITHM DESIGN</b> Came up with a way to \'featurize\' images to efficiently compare them</li></ul></p><p>The entire program was written in <b>Python</b>. I chose Python because it\'s got an awesome abstraction for multiprocess programming that makes it quick write highly parallel programs.</p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center proxee-text-col"><h1 class=lead>PIPELINE</h1><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class=dedup-limit src=assets/images/dedup-architecture.tiff alt=""><h6><b>Figure 1:</b> The deduplication pipeline.</h6><br></div></div><br><div id=roommate class="row proxee-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the deduplication pipeline. This architecture allowed me to write a program that can deduplicate <b>~70000</b> images in about <b>10 seconds</b>.</p><br><h4 class=text-center><b>Techie Section:</b> What\'s happening behind the scenes?</h4><p><ul><li>Near De-duplication</li><li>Exact De-duplication</li><li>"MapReduce"</li></ul>It was extremely necessary to write a program that could deduplicate efficiently. No one wants to wait forever for a task such as this. To speed up the process, the python program:<ul><li><b>Split</b> Splits the data into M smaller segments</li><li><b>Workers</b> Creates M worker processes</li><li><b>Deduplicate</b> Tells each worker process to deduplicate it\'s smaller data chunk</li><li><b>Merge</b> Then merge the deduplicated images from the smaller sets</li></ul></p><p>The deduplication has two methodologies. One is \'exact\' deduplication, and the other is \'near\' deduplication. An exact duplicate set of images would have all the same exact pixels in the same exact positions. A nearly duplicate set of images could be the same images with slightly differing brightness, or slightly different pixel values.</p><h4>Exactly!</h4><p>Exact deduplication can be done fairly quickly, by simply \'hashing\' the image bytes and storing the hash/value pairs in a lookup table/dictionary. This ends up being extremely efficient, and you can iterate and deduplicate the data in one pass. Then add that to the fact that it\'s working on multiple cores/processes and you get a blazingly fast program.</p><h4>The end is Nigh... Near?</h4><p>Near deduplication is slightly different. In this case, you can\'t just simply \'hash\' the image. If the images are different in ANY way, and you hash them, the resultant hashes will be unbelievably different. That\'s a desired result of hashing, but it doesn\'t suit the problem we\'re trying to solve.</p><p>It would be nice if we could get similar hashes when the items were only slightly different. Turns out that some awesome people at Google came up with what\'s called <code>Simhash</code>. Essentially, it gives us the ability to reduce a huge piece of data (an image) to a small \'fingerprint\'! In addition to reducing the data, it also lets two pieces of data that are somewhat \'similar\' to have similar hashes (aka sim... hashes...). To generate the finger print I used features such as the image height, width, portions of the image bytes, etc.</p><p>I applied a similar high level logic to solving the near de-duplication problem. Instead of simply seeing if a hash value exists within the lookup table, I check to see if a \'nearly\' similar hash value exists within the lookup table. To speed this up even more, I use an index which sits on top of the data.</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/liltron.html",'<div class="jumbotron liltron-jumbotron text-center liltron-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}}</h3></div><div id=images class="row liltron-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h1 class="lead text-center">OVERVIEW</h1><p>Lil\' Tron is a project that two other students and myself built as apart of an Applied Natural Language Processing class. Our goal was to build a program which could generate cool rap lyrics on it\'s own! The app consisted of a front end web app, an artificially intelligent backend.</p><h1 class=lead>WHAT I DID</h1><p>My role in this project was:<ul><li><b>FRONT END:</b> Create a <b>AngularJS</b> front end web app to display auto generated rap lyrics. Grab lyrics from other rap bot websites. Create a voting system to let people choose between our lyrics and competitor\'s lyrics.</li><li><b>BACK END:</b> Create a <b>NodeJS</b> & <b>Python</b> back end to run the Natural Language Processing algorithms my team and I built.</li></ul></p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h1 class=lead>THE LOOK</h1><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class=liltron-limit src=assets/images/liltron-iphone.tiff alt=""><h6><b>Figure 1:</b> The Lil Tron web app.</h6><br></div></div><br><div id=roommate class="row liltron-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> The app gives a simple interface. Users could press the \'Rhymify\' button and be presented with two sets of lyrics. Our generated lyrics, and lyrics from another website. They then had the choice of choosing their \'Favorite\' lyric.</p><br><h4 class=text-center><b>Techie Section:</b> What\'s happening behind the scenes?</h4><p><ul><li>AngularJS Web App (Me)</li><li>NodeJS Web Server (Me)</li><li>Python NLP Server (Team)</li><li>MongoDB Persistence (Me)</li></ul></p><h4>NLP Engine</h4><p>Let\'s start from the core. The NLP engine for generating Rap lyrics. To build this engine we needed three things:<ul><li>Language Model (Teammate 1 - Akshay)</li><li>Rhyming \'Goodness\' (Teammate 2 - Gandahli)</li><li>Thesaurus (Me)</li></ul>These three steps were implemented in <b>Python</b>. I\'ll first talk about how we accomplished each task. Then I\'ll talk about how each of the pieces worked together.</p><h5 class="lead text-center">N-GRAM</h5><p>Akshay crawled the lyrics of about 10000 rap songs, and generated an NGram language model. To put it briefly, an ngram language model stores phrases of words, and a probability score (from 0 - 1) for how likely it is to see those phrases. The \'N\' in Ngram says how many words per phrase you store. The NGram allows us to generate words based on previous words seen.</p><h5 class="lead text-center">RHYMING \'GOODNESS\'</h5><p>The next thing that is important, is the notion of rhyming. Generating sentences is cool, but for rap to sound good, we need rhyming words. To do this, Gandhali wrote a program to grab rhyming scores from a website; the website gives numerical scores for how well words rhyme with eachother. So given a word, we have a list of words that rhyme with it, as well as scores for how well each word rhymes.</p><h5 class="lead text-center">RELATED WORDS</h5><p>The last piece is to add some variability to the words the Ngram language model uses. The NGram generates words based on what it has seen already (it\'s vocabulary). The rhyming portion lets you know how well two words rhyme. But now you need add some spontaneity to the rhymes so they don\'t seem stale. This is where a thesaurus comes in. A thesaurus tells you the synonyms for a given word. To add more similar words to the one in our vocabulary, I created a program to grab words from thesaurus websites and store them in a JSON dictionary.</p><h5 class="lead text-center">PUT IT ALL TOGETHER</h5><p>Now that we have all the pieces, let\'s talk about how they fit together.<ol><li>Generate 18 words via the NGram language model. This will be a sentence (aka 2 bars)</li><li>Break the sentence into two pieces</li><li>Look at the last words of piece 1 and piece 2. We\'ll call them "word-1" and "word-2".</li><li>Use the thesaurus to find the related words/synonyms for "word-1" and "word-2". We\'ll call those "related-list-1" and "related-list-2", respectively. Also use part of speech tagging to make sure nouns match with nouns, adjectives with adjectives, etc.</li><li>Iterate through the combinations of words and find the pair that gives you the highest rhyme score</li><li>Put replace "word-1" and "word-2" with the pair of words you found!</li><li>Repeat this 3 times to get 16 bars!</li></ol></p><br><h5 class="lead text-center">SUMMARY</h5><p>Great so now that we\'ve discussed the NLP engine, let\'s talk about the rest of it; making an application out of this. The goal was to host this online so that peers within our class could try out our application. I was faced with the task of deploying this online.</p><p>I wrote an AngularJS web app to query the backend for rap lyrics. I also wrote the backend server(s) to handle requests. The backend was composed of three pieces:<ul><li>NodeJS webserver</li><li>Python NLP Server</li><li>MongoDB Persistence</li></ul>The NodeJS webserver handled requests from the AngularJS web app. Once the user pressed the \'rhymify\' button on the webapp, the NodeJS server queries the python NLP engine server for lyrics. Once the lyrics are generated, the NodeJS server returns the lyrics back to the web app. All of this was stored on Amazon Web Services. To keep the app running quickly, I stored the language model, thesaurus and rhyme dictionary in memory. This ended up being a little tricky at first because the files were larger then the provided ram (> 4 GB). To get around this I increased the virtual memory (via swap file) and things worked like a charm! It takes about <b>10 seconds</b> from the time \'rhymify\' is clicked to return lyrics to the user! We could definitely have sped this up via multiprocessing or distributing the load, but we also had to implement this within a couple of weeks. Not bad if you ask me!</p><p>The last step happens when a user chooses a \'favorite\' lyric. We store the lyric they chose as well as which site generated the lyric (ours, or the competitor). We give no indication which lyric is ours or the competitors. This was our validation test; it let us see if we were generating good lyrics. After leaving the voting open for a few days, our lyrics <b>outperformed</b> the competitor!</p></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/mega.html",'<div class="jumbotron text-center mega-vertical-center mega-jumbotron-bg"><div class=container><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}}</h3><h4 class="wow fadeInUp">{{projectsController.project.date}}</h4></div></div><div class="container-fluid wow fadeInUp"><div id=overview class="row proxee-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h1 class="lead text-center">OVERVIEW</h1><p><a ng-href={{projectsController.project.url}} target=_blank>The Code</a></p><p>When I started getting into web design, I built a website for a student organization called MEGA (Minority Engineering Graduate Association) at USC. At the time, I was the president of the organization, and I was interested in revamping the website.</p><p>I built the mockup website using <b>AngularJS</b>, and <b>Twitter Bootstrap</b>. To make development efficient, I used a command line tools called <code>Grunt</code> and <code>Karma</code>. What do they do?<ul><li><b>Grunt:</b> A task runner that allows you to automate things. Really useful for \'minifying\' and \'uglifying\' your css and js files. Basically that boils down to shrinking your code so that it takes your web browser less time to download and render your website!</li><li><b>Karma:</b> An awesome test harness that allows you to test your JS</li></ul></p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center proxee-text-col"><h1 class=lead>THE RESULT</h1><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class=mega-limit src=assets/images/mega-website-wide.tiff alt=""><h6><b>Figure 1:</b> The main page for the MEGA website.</h6><br></div></div><br><div id=roommate class="row proxee-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the final result for the MEGA website. I really wanted to make a simple page that highlighted the most important things about MEGA. The first thing a person would see when they went to the website would be a <code>Carousel</code> aka an <code>Image Slider</code>. The image slider showcased us at social events, community service events and also at professional events like the NSBE national convention. Additionally, I leveraged Twitter Bootstrap to make the website responsive, so that it would look nice on a screen of any size!</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/olts.html",'<div class="jumbotron olts-jumbotron text-center olts-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center olts-text-col"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}} - PhD Thesis Work</h3><h4>{{projectsController.project.date}}</h4></div><div id=images class="row olts-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h1 class="lead text-center">OVERVIEW</h1><p>Those interested in code check it out on <a target=_blank href={{projectsController.project.url}}>Github</a>!</p><p>OLTS is a <b>C++</b> based system that I built as apart of my PhD thesis. OLTS stands for Object Tracking and Localization System. It is one module of a tool called the "Wearable Visual Aid" (see Figure 1) to help the blind do daily tasks. The system is composed of three pieces:<ul><li>Head Mounted Camera</li><li>Computer</li><li>Headphones/Vibration Motors</li></ul></p><p>First a camera fed visual information to the computer. The computer contains algorithms that can track an object real time. The objects in this case are grocery store items. While tracking the object, the computer generates speech or vibrotactile feedback based on the objects position. So essentially, you get a computer telling the blind person how to reach and grasp for grocery store items!</p><h1 class=lead>WHAT I DID</h1><p>My goal was to build a proof of concept system to be tested with visually impaired people. To do this I did a few things:<ul><li><b>HUMAN SUBJECTS TESTING:</b> It\'s really awesome building stuff for people. What\'s even cooler is letting people try what you\'ve built. I tested the system I built with 13 visually impaired subjects</li><li><b>SOFTWARE DESIGN:</b> I needed to figure out an efficient way to make the computer vision modules work with the speech/vibration feedback modules.</li></ul></p><p>After testing with the 13 visually impaired subjects, we determined that subjects were able to find the correct item (out of 3-5 items) within <b>20 seconds</b>.</p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h1 class=lead>THE WEARABLE VISUAL AID</h1><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class=olts-limit src=assets/images/wva-flowchart.tiff alt=""><h6><b>Figure 1:</b> The Wearable Visual Aid. A proof of concept closed loop grocery shopping assistant for visually impaired people.</h6><br></div></div><br><div id=olts-section2 class="row olts-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the Wearable Visual Aid (WVA). It\'s a closed loop system that:<ul><li>Lets a visually impaired person choose a grocery store item via smartphone</li><li>Uses computer vision based object recognition to find items in video</li><li>Generate real time feedback based on the object\'s position</li></ul></p><p>The WVA was an interdisciplinary project. My system, OLTS was one piece of the WVA system.</p><br><h4 class=text-center><b>Techie Section: Under the Hood</b></h4><p>The OLTS is a multithreaded OpenCV C++ program. It\'s run as a command line application. One of the libraries I\'m using is Windows only and proprietary, so it only runs on Windows! I\'m using Boost threading library to make the program run in real time. The program gives an option for running in speech mode, or vibrotactile mode. In both cases, the structure of the program is as follows:<ul><li><b>Thread 0:</b> Main Thread</li><li><b>Thread 1:</b> Vision Thread</li><li><b>Thread 2:</b> Feedback Thread</li><li><b>Thread 3:</b> Data Logging Thread</li></ul></p><p><b>Main Thread: </b>The main thread is responsible for handling configuration options like which feedback mode, or even parameters for the feedback portion of the program. At the start of the program, the main thread starts a GUI to select a region within an image. This gives the researcher (me) a chance to draw a box around the grocery item of interest. Once I\'ve done that, then I let the program run autonomously and speak to the visually impaired person. The main thread is also responsible for creating the vision, feedback and data logging threads.</p><p><b>Vision Thread: </b>The vision thread is responsible for grabbing frames from the camera and also taking care of doing computer vision processing. Specifically, the computer vision algorithm is an Object Tracking program called the <a href=http://iris.usc.edu/people/thangdin/research.html target=_blank>Context Tracker</a>. The tracking algorithm gives us real time positional updates of the selected item and stores this in a shared variable.</p><p><b>Feedback Thread: </b>The feedback thread is responsible reading from the position shared variable. Upon reading from the shared variable, it generates vibration or speech based on the position in the field of view. Essentially, we split the camera frame into 9 distinct regions. I call it the "Sensory Map" (see figure 2). If an object falls in a region, then the feedback thread acts accordingly. The words generated by the computer are simple commands like "Up", "Down", etc. The vibrations come from 4 motors (M1 - M4). We place these motors on the headphones as well.</p><p><b>Data Logging Thread: </b>The data logging thread records what the camera is seeing as well as the tracking path of the object over time.</p><br></div><div class="col-md-2 col-lg-2"></div></div><div class="row text-center olts-text-col"><h1 class=lead>THE SENSORY MAP</h1><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class="olts-limit text-center" src=assets/images/sensory-map.tiff alt="Sensory Map"><h6><b>Figure 2:</b> The Sensory Map. This represents the camera\'s field of view. If an item falls in a certain region/box then the computer speaks or vibrates the command.</h6><br></div></div></div>'),
e.put("app/model/projects/proxee.html",'<div class="jumbotron project-jumbotron text-center proxee-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center"><h1 class="wow fadeInDown">PROXEE</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}}</h3></div><div id=images class="row proxee-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h1 class="lead text-center">OVERVIEW</h1><p>Proxee is an iOS app I started developing during the summer of 2015. A friend and I had an idea about making it easier for people to not only find great places to live, but to also find great people to live with.</p><h1 class=lead>WHAT I DID</h1><p>My role in this project was:<ul><li><b>DESIGN:</b> Coming up with a slick design and UX for the app</li><li><b>FRONT END:</b> Writing the app code and and bringing the design to life</li><li><b>BACK END:</b> I wired up a simple backend on Parse to store user & apartment data</li></ul></p><p>To write the app, I picked up <b>Swift</b>. I hadn\'t programmed in Swift before, but I had experience in Objective-C prior to that. It was a really smooth transition from Obj-C to Swift. (I think Apple did an amazing job btw with the language!) We never launched the app to the app store, but it was a great experience writing the app. I\'m looking forward to the day when I completely launch my first app!</p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h1 class=lead>THE LOOK</h1><div class="col-xs-12 col-sm-12 col-md-6 col-lg-6"><img class=project-limit src=assets/images/proxee.png alt=""><h6><b>Figure 1:</b> The apartment listing feed.</h6><br></div><div class="col-xs-12 col-sm-12 col-md-6 col-lg-6"><img class=project-limit src=assets/images/proxee-roommate.png alt=""><h6><b>Figure 2:</b> Like Tinder... for roommate matching</h6><br></div></div><br><div id=roommate class="row proxee-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows the apartment listing feed. The goal for this page was to make the focus all on the actual listings themselves. I took inspiration from Pinterest, and chose what\'s called a \'waterfall\' layout to let each apartment listing flow into the next. I was really interested in minimizing the details (price, bed/bath, etc.) just enough. Just enough so that they\'re legible, but not to large that they take focus off the real content.</p><br><h4 class=text-center><b>Techie Section:</b> What\'s happening behind the scenes?</h4><p><ul><li>Smooth Scrolling</li><li>MVVM - Model - View - View Model</li></ul>To make scrolling really fast and responsive I needed to asynchronously load images. Any task that required network IO was off loaded to background tasks. Definitely a standard practice to do this, but I think it\'s extremely important to note that fact.</p><p>To make it easy to manipulate apartment listing data and also create a separation of concerns between data and presentation, I adopted a design paradigm called MVVM (Model - View - View Model). I chose this instead of the standard MVC (Model - View - Controller) to avoid really massive view controllers. Essentially, the MVC paradigm often tends to lead to Controllers that do EVERYTHING. Network calls, loading data into views, etc. MVVM is a nice way to separate data manipulation and view logic. Additionally, it allows you to reuse a lot of the same components extremely easily accross multiple views.</p><p><b>Figure 2</b> shows the roommate matching portion of the app. My logic for this page was to use a layout that most of our target demographic would be familiar with. People in their 20\'s are fairly aware of dating apps, so a Tinder like interface is both familiar and easy to use for that demo. \'Swipe Right\' if you think you could live with that person and swipe... yeah you get the idea.</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>'),e.put("app/model/projects/visor.html",'<div class="jumbotron visor-jumbotron text-center visor-jumbotron-bg wow fadeInDown"></div><div class="container-fluid wow fadeInUp"><div class="row text-center visor-text-col"><h1 class="wow fadeInDown">{{projectsController.project.title}}</h1><h3 class="lead wow fadeInUp">{{projectsController.project.description}}</h3><h4>{{projectsController.project.date}}</h4></div><div id=images class="row visor-text-col"><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div><div class="col-md-8 col-lg-8 project-content"><h1 class="lead text-center">OVERVIEW</h1><p>VISOR.AI is a deep learning module that I built as apart of my PhD Thesis. VISOR is a play on the words <b>O</b>bject <b>R</b>ecognition for the <b>VIS</b>ually impaired. The specific purpose of VISOR.AI was to recognize grocery store items in images. This module fits into a larger system that aims to help visually impaired people with daily tasks. In this case that daily task is grocery shopping.</p><h1 class=lead>WHAT I DID</h1><p>My goal for this project is to accomplish object recognition in real time. To do this I\'m doing the following:<ul><li><b>DATA PIPELINE:</b> One of the biggest pieces of a good neural net is the data. I created a pipeline to grab grocery store images and preprocess them.</li><li><b>NETWORK DESIGN:</b> I\'m coming up with a compact deep learning net</li><li><b>VISUALIZATION:</b> It\'s important to see how well your network is doing, as well as what your features look like. I\'m also writing code to accomplish this</li></ul></p><p></p></div><div class="col-xs-12 col-sm-12 col-md-2 col-lg-2"></div></div><div class="row text-center"><h1 class=lead>THE NETWORK</h1><div class="col-xs-12 col-sm-12 col-md-12 col-lg-12"><img class=visor-limit src=assets/images/visor-network.tiff alt=""><h6><b>Figure 1:</b> Version 1 of the VISOR.AI neural network.</h6><br></div></div><br><div id=roommate class="row visor-text-col"><br><div class="col-md-2 col-lg-2"></div><div class="col-xs-12 col-sm-12 col-md-8 col-lg-8 text-justify"><p><b>Figure 1</b> shows a network diagram for the VISOR.AI network. The system takes in a 50x50 pixel image, and ultimately gives a guess for category of the item. Right now there are 8000 items that the network has been trained on. An example of an item could be a box of \'Cheerios\'. These 8000 items all fall into some category. So \'Cheerios\' would fall into a category such as \'Food/Cereal\'! At the moment, the network processes an image in <b>9 milliseconds</b>. The overall accuracy of the system is <b>~43%</b>.</p><br><h4 class=text-center><b>Current Work</b></h4><p>This project is currently in progress. Currently, I am:<ul><li>Gathering more grocery images to refine the data used for training.</li><li>Trying different image resolutions greater than 50x50 pixels</li><li>Refining parameters such as types & number of layers</li><li>Refining hyperparameters like learning rate, epochs, momentum</li></ul>I will update this page as I make progress. Stay tuned!</p><br></div><div class="col-md-2 col-lg-2"></div></div></div>')}]);
//# sourceMappingURL=../maps/scripts/app-4b1c7b4685.js.map
